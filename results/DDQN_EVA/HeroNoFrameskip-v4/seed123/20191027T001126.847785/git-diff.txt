diff --git a/dkuyoshi/eva.py b/dkuyoshi/eva.py
index 71db817..3ca5628 100644
--- a/dkuyoshi/eva.py
+++ b/dkuyoshi/eva.py
@@ -1,267 +1,238 @@
-from __future__ import unicode_literals
-from __future__ import print_function
-from __future__ import division
-from __future__ import absolute_import
-from future import standard_library
-
-standard_library.install_aliases()  # NOQA
-
-import os
-from logging import getLogger
-import numpy as np
-
-import chainer
-from chainer import cuda
-from chainerrl.agents import dqn
-from chainerrl.misc.batch_states import batch_states
-from chainerrl.recurrent import Recurrent
-from chainerrl.replay_buffer import batch_experiences
-
-import copy
-from chainerrl.misc.copy_param import synchronize_parameters
-from chainerrl.recurrent import state_kept
-
-
-class EVA(dqn.DQN):
-    """EVAアルゴリズム(DQNに適用)"""
-    def __init__(self, q_function, optimizer, replay_buffer, gamma,
-                 explorer, gpu=None, replay_start_size=40000,
-                 minibatch_size=48, update_interval=1,
-                 target_update_interval=100, clip_delta=True,
-                 phi=lambda x: x,
-                 target_update_method='hard',
-                 soft_update_tau=1e-2,
-                 n_times_update=1, average_q_decay=0.999,
-                 average_loss_decay=0.99,
-                 batch_accumulator='mean', episodic_update=False,
-                 episodic_update_len=100,
-                 logger=getLogger(__name__),
-                 batch_states=batch_states,
-                 len_trajectory=50,
-                 periodic_steps=20
-                 ):
-        super().__init__(q_function, optimizer, replay_buffer, gamma,
-                         explorer, gpu, replay_start_size=replay_start_size,
-                         minibatch_size=minibatch_size, update_interval=update_interval,
-                         target_update_interval=target_update_interval, clip_delta=clip_delta,
-                         phi=phi,
-                         target_update_method=target_update_method,
-                         soft_update_tau=soft_update_tau,
-                         n_times_update=n_times_update, average_q_decay=average_q_decay,
-                         average_loss_decay=average_loss_decay,
-                         batch_accumulator=batch_accumulator, episodic_update=episodic_update,
-                         episodic_update_len=episodic_update_len,
-                         logger=logger,
-                         batch_states=batch_states)
-
-        # 必要なパラメータ記述
-        self.last_embed = None
-        self.len_trajectory = len_trajectory
-        self.num_actions = self.model.num_actions
-        self.periodic_steps = periodic_steps
-        self.value_buffer = self.model.non_q
-        self.current_t = 0
-
-    def act_and_train(self, obs, reward):
-        with chainer.using_config('train', False), chainer.no_backprop_mode():
-            action_value = self.model(
-                self.batch_states([obs], self.xp, self.phi), eva=(self.t >= self.replay_buffer.capacity) and (len(self.value_buffer.embeddings) == self.value_buffer.capacity))
-            q = float(action_value.max.array)
-            greedy_action = cuda.to_cpu(action_value.greedy_actions.array)[0]
-            embed = cuda.to_cpu(self.model.get_embedding().array)
-
-        # Update stats
-        self.average_q *= self.average_q_decay
-        self.average_q += (1 - self.average_q_decay) * q
-
-        self.logger.debug('t:%s q:%s action_value:%s', self.t, q, action_value)
-
-        action = self.explorer.select_action(
-            self.t, lambda: greedy_action, action_value=action_value)
-        self.t += 1
-
-        # Update the target network
-        if self.t % self.target_update_interval == 0:
-            self.sync_target_network()
-
-        if self.last_state is not None:
-            assert self.last_action is not None
-            # Add a transition to the replay buffer
-
-            # TODO: DQNのままなので, 必要なものを入れるようにちゃんと変更する
-
-            # 必要なもの追加
-            self.replay_buffer.append(
-                state=self.last_state,
-                action=self.last_action,
-                reward=reward,
-                embedding=self.last_embed[0],
-                next_state=obs,
-                next_action=action,
-                is_state_terminal=False)
-
-        self.last_state = obs
-        self.last_action = action
-        self.last_embed = embed
-
-        self.replay_updater.update_if_necessary(self.t)
-        self.backup_store_if_necessary(embed, self.t)
-
-        self.logger.debug('t:%s r:%s a:%s', self.t, reward, action)
-
-        return self.last_action
-
-    def backup_store_if_necessary(self, embedding, t):
-        if (t % self.periodic_steps == 0) and (len(self.replay_buffer) == self.replay_buffer.capacity):
-            trajectories = self.replay_buffer.lookup(embedding)
-            embeddings1 = []
-            #軌跡ごとに羅列
-            batch_trajectory = [{'state': batch_states([elem[0]['state'] for elem in traject], self.xp, self.phi),
-                                  'action': np.asarray([elem[0]['action'] for elem in traject], dtype=np.int32),
-                                  'reward': np.asarray([elem[0]['reward'] for elem in traject], dtype=np.float32),
-                                  'embedding': [elem[0]['embedding'] for elem in traject]
-                                  } for traject in trajectories]
-
-            qnp, embeddings = self._trajectory_centric_planning(batch_trajectory)
-            self.value_buffer.store(embeddings, qnp)
-
-    def _trajectory_centric_planning(self, trajectories):
-        """TODO: 経験の軌跡からTCPでそれぞれのvalueを計算する関数.
-        これは外から呼び出さない(private)
-        """
-        # TCPの実装
-        #初期化
-        Q_value = np.empty((0, self.num_actions), dtype=np.float32)
-        embeddings = []
-        for trajectory in trajectories:
-            state = trajectory['state']
-            embeddings += trajectory['embedding']
-            with chainer.using_config('train', False), chainer.no_backprop_mode():
-                parametric_q = self.model(self.xp.array(state, dtype=self.xp.float32), eva=False)
-                parametric_q = parametric_q.q_values.array
-
-            action = trajectory['action']
-            reward = trajectory['reward']
-            T = len(action)
-            Qnp = parametric_q
-            Vnp = np.max(parametric_q[T - 1])
-            for t in range(T - 2, -1, -1):
-                Qnp[t][action[t]] = reward[t] + self.gamma * Vnp
-                Vnp = np.max(Qnp[t])
-            #軌跡ごとに追加
-            Q_value = np.vstack((Q_value, Qnp))
-
-        # それぞれのvalue, embeddingを出力
-        return Q_value, embeddings
-
-
-    '''def _trajectory_centric_planning(self, trajectories):
-        embeddings = []
-        batch_state = self.xp.empty((0, 4, 84, 84), dtype=self.xp.float32)
-        for trajectory in trajectories:
-            shape = self.xp.empty((self.len_trajectory, 4, 84, 84), dtype=self.xp.float32)
-            shape[:len(trajectory['state'])] = trajectory['state']
-            batch_state = self.xp.vstack((batch_state, shape))
-            embeddings += trajectory['embedding']
-
-        with chainer.using_config('train', False), chainer.no_backprop_mode():
-            parametric_q = self.target_model(batch_states)
-            parametric_q = cuda.to_cpu(parametric_q.q_values.array).reshape((len(trajectories), self.len_trajectory , self.num_actions))
-
-        q_value = np.empty((0, self.num_actions), dtype=np.float32)
-        for qnp, trajectory in zip(parametric_q, trajectories):
-            action = trajectory['action']
-            reward = trajectory['reward']
-            T = len(action)
-            qnp = qnp[:T]
-            Vnp = np.max(qnp[T - 1])
-            for t in range(T - 2, -1, -1):
-                qnp[t][action[t]] = reward[t] + self.gamma * Vnp
-                Vnp = np.max(qnp[t])
-            q_value = np.vstack((q_value, qnp))
-
-        return q_value, embeddings
-        '''
-
-    def act(self, obs):
-        with chainer.using_config('train', False), chainer.no_backprop_mode():
-            action_value = self.model(
-                self.batch_states([obs], self.xp, self.phi), eva=(self.t >= self.replay_buffer.capacity) and (len(self.value_buffer.embeddings) == self.value_buffer.capacity))
-            q = float(action_value.max.array)
-            action = cuda.to_cpu(action_value.greedy_actions.array)[0]
-            embed = cuda.to_cpu(self.model.get_embedding().array)
-
-        # Update stats
-        self.average_q *= self.average_q_decay
-        self.average_q += (1 - self.average_q_decay) * q
-        self.logger.debug('t:%s q:%s action_value:%s', self.t, q, action_value)
-
-        self.backup_store_if_necessary(embed, self.t)
-        self.current_t += 1
-        return action
-
-    def sync_target_network(self):
-        """Synchronize target network with current network."""
-        if self.target_model is None:
-            self.target_model = copy.deepcopy(self.model.q_func)
-            call_orig = self.target_model.__call__
-
-            def call_test(self_, x):
-                with chainer.using_config('train', False):
-                    return call_orig(self_, x)
-
-            self.target_model.__call__ = call_test
-        else:
-            synchronize_parameters(
-                src=self.model.q_func,
-                dst=self.target_model,
-                method=self.target_update_method,
-                tau=self.soft_update_tau)
-
-    def stop_episode_and_train(self, state, reward, done=False):
-        assert self.last_state is not None
-        assert self.last_action is not None
-        assert self.last_embed is not None
-
-        self.replay_buffer.append(
-            state=self.last_state,
-            action=self.last_action,
-            reward=reward,
-            embedding=self.last_embed[0],
-            next_state=state,
-            is_state_terminal=done,
-        )
-
-        self.replay_updater.update_if_necessary(self.t)
-        self.backup_store_if_necessary(self.last_embed, self.t)
-        self.stop_episode()
-
-    def stop_episode(self):
-        self.last_state = None
-        self.last_action = None
-        self.last_embed = None
-        self.current_t = 0
-
-        if isinstance(self.model, Recurrent):
-            self.model.reset_state()
-
-        self.replay_buffer.stop_current_episode()
-
-class EVADoubleDQN(EVA):
-
-    def _compute_target_values(self, exp_batch):
-        batch_next_state = exp_batch['next_state']
-
-        with chainer.using_config('train', False), state_kept(self.q_function):
-            next_qout = self.q_function(batch_next_state)
-
-        target_next_qout = self.target_q_function(batch_next_state)
-
-        next_q_max = target_next_qout.evaluate_actions(
-            next_qout.greedy_actions)
-
-        batch_rewards = exp_batch['reward']
-        batch_terminal = exp_batch['is_state_terminal']
-        discount = exp_batch['discount']
-
-        return batch_rewards + discount * (1.0 - batch_terminal) * next_q_max
+from __future__ import unicode_literals
+from __future__ import print_function
+from __future__ import division
+from __future__ import absolute_import
+from future import standard_library
+
+standard_library.install_aliases()  # NOQA
+
+import os
+from logging import getLogger
+import numpy as np
+
+import chainer
+from chainer import cuda
+from chainerrl.agents import dqn
+from chainerrl.misc.batch_states import batch_states
+from chainerrl.recurrent import Recurrent
+from chainerrl.replay_buffer import batch_experiences
+
+import copy
+from chainerrl.misc.copy_param import synchronize_parameters
+from chainerrl.recurrent import state_kept
+import time
+
+class EVA(dqn.DQN):
+    """EVAアルゴリズム(DQNに適用)"""
+    def __init__(self, q_function, optimizer, replay_buffer, gamma,
+                 explorer, gpu=None, replay_start_size=40000,
+                 minibatch_size=48, update_interval=1,
+                 target_update_interval=100, clip_delta=True,
+                 phi=lambda x: x,
+                 target_update_method='hard',
+                 soft_update_tau=1e-2,
+                 n_times_update=1, average_q_decay=0.999,
+                 average_loss_decay=0.99,
+                 batch_accumulator='mean', episodic_update=False,
+                 episodic_update_len=100,
+                 logger=getLogger(__name__),
+                 batch_states=batch_states,
+                 len_trajectory=50,
+                 periodic_steps=20
+                 ):
+        super().__init__(q_function, optimizer, replay_buffer, gamma,
+                         explorer, gpu, replay_start_size=replay_start_size,
+                         minibatch_size=minibatch_size, update_interval=update_interval,
+                         target_update_interval=target_update_interval, clip_delta=clip_delta,
+                         phi=phi,
+                         target_update_method=target_update_method,
+                         soft_update_tau=soft_update_tau,
+                         n_times_update=n_times_update, average_q_decay=average_q_decay,
+                         average_loss_decay=average_loss_decay,
+                         batch_accumulator=batch_accumulator, episodic_update=episodic_update,
+                         episodic_update_len=episodic_update_len,
+                         logger=logger,
+                         batch_states=batch_states)
+
+        # 必要なパラメータ記述
+        self.last_embed = None
+        self.len_trajectory = len_trajectory
+        self.num_actions = self.model.num_actions
+        self.periodic_steps = periodic_steps
+        self.value_buffer = self.model.non_q
+        self.current_t = 0
+
+    def act_and_train(self, obs, reward):
+        with chainer.using_config('train', False), chainer.no_backprop_mode():
+            action_value = self.model(
+                self.batch_states([obs], self.xp, self.phi), eva=(len(self.value_buffer.embeddings) == self.value_buffer.capacity))
+            q = float(action_value.max.array)
+            greedy_action = cuda.to_cpu(action_value.greedy_actions.array)[0]
+            embed = cuda.to_cpu(self.model.get_embedding().array)
+
+        # Update stats
+        self.average_q *= self.average_q_decay
+        self.average_q += (1 - self.average_q_decay) * q
+
+        self.logger.debug('t:%s q:%s action_value:%s', self.t, q, action_value)
+
+        action = self.explorer.select_action(
+            self.t, lambda: greedy_action, action_value=action_value)
+        self.t += 1
+
+        # Update the target network
+        if self.t % self.target_update_interval == 0:
+            self.sync_target_network()
+
+        if self.last_state is not None:
+            assert self.last_action is not None
+            # Add a transition to the replay buffer
+
+            # TODO: DQNのままなので, 必要なものを入れるようにちゃんと変更する
+
+            # 必要なもの追加
+            self.replay_buffer.append(
+                state=self.last_state,
+                action=self.last_action,
+                reward=reward,
+                embedding=self.last_embed[0],
+                next_state=obs,
+                next_action=action,
+                is_state_terminal=False)
+
+        self.last_state = obs
+        self.last_action = action
+        self.last_embed = embed
+
+        self.replay_updater.update_if_necessary(self.t)
+        self.backup_store_if_necessary(embed, self.t)
+
+        self.logger.debug('t:%s r:%s a:%s', self.t, reward, action)
+
+        return self.last_action
+
+    def backup_store_if_necessary(self, embedding, t):
+        if (t % self.periodic_steps == 0) and (self.t >= self.replay_buffer.capacity):
+            self.replay_buffer.update_embedding()
+            trajectories = self.replay_buffer.lookup(embedding)
+            batch_trajectory = [{'state': batch_states([elem[0]['state'] for elem in traject], self.xp, self.phi),
+                                  'action': [elem[0]['action'] for elem in traject],
+                                  'reward': [elem[0]['reward'] for elem in traject],
+                                  'embedding': [elem[0]['embedding'] for elem in traject]
+                                  } for traject in trajectories]
+
+            qnp, embeddings = self._trajectory_centric_planning(batch_trajectory)
+            self.value_buffer.store(embeddings, qnp)
+
+    def _trajectory_centric_planning(self, trajectories):
+        #atari
+        embeddings = []
+        batch_state = []
+        for trajectory in trajectories:
+            embeddings += trajectory['embedding']
+            batch = self.xp.empty((self.len_trajectory, 4, 84, 84), dtype=self.xp.float32)
+            batch[:len(trajectory['state'])] = trajectory['state']
+            batch_state.append(batch)
+
+        batch_state = self.xp.concatenate(batch_state, axis=0).astype(self.xp.float32)
+
+        with chainer.using_config('train', False), chainer.no_backprop_mode():
+            parametric_q = self.model(batch_state, eva=False)
+            parametric_q = cuda.to_cpu(parametric_q.q_values.array).reshape((len(trajectories), self.len_trajectory , self.num_actions))
+
+        q_value = []
+        for qnp, trajectory in zip(parametric_q, trajectories):
+            action = trajectory['action']
+            reward = trajectory['reward']
+            T = len(action)
+            qnp = qnp[:T]
+            Vnp = np.max(qnp[T - 1])
+            for t in range(T - 2, -1, -1):
+                qnp[t][action[t]] = reward[t] + self.gamma * Vnp
+                Vnp = np.max(qnp[t])
+            q_value.append(qnp)
+            
+        return self.xp.asarray(np.concatenate(q_value, axis=0).astype(np.float32)), self.xp.asarray(embeddings)
+        
+    def act(self, obs):
+        with chainer.using_config('train', False), chainer.no_backprop_mode():
+            action_value = self.model(
+                self.batch_states([obs], self.xp, self.phi), eva=(len(self.value_buffer.embeddings) == self.value_buffer.capacity))
+            q = float(action_value.max.array)
+            action = cuda.to_cpu(action_value.greedy_actions.array)[0]
+            embed = cuda.to_cpu(self.model.get_embedding().array)
+
+        # Update stats
+        self.average_q *= self.average_q_decay
+        self.average_q += (1 - self.average_q_decay) * q
+        self.logger.debug('t:%s q:%s action_value:%s', self.t, q, action_value)
+
+        self.backup_store_if_necessary(embed, self.current_t)
+        self.current_t += 1
+        return action
+
+    def sync_target_network(self):
+        """Synchronize target network with current network."""
+        if self.target_model is None:
+            self.target_model = copy.deepcopy(self.model.q_func)
+            call_orig = self.target_model.__call__
+
+            def call_test(self_, x):
+                with chainer.using_config('train', False):
+                    return call_orig(self_, x)
+
+            self.target_model.__call__ = call_test
+        else:
+            synchronize_parameters(
+                src=self.model.q_func,
+                dst=self.target_model,
+                method=self.target_update_method,
+                tau=self.soft_update_tau)
+
+    def stop_episode_and_train(self, state, reward, done=False):
+        assert self.last_state is not None
+        assert self.last_action is not None
+        assert self.last_embed is not None
+
+        self.replay_buffer.append(
+            state=self.last_state,
+            action=self.last_action,
+            reward=reward,
+            embedding=self.last_embed[0],
+            next_state=state,
+            is_state_terminal=done,
+        )
+
+        self.replay_updater.update_if_necessary(self.t)
+        self.backup_store_if_necessary(self.last_embed, self.t)
+        self.stop_episode()
+
+    def stop_episode(self):
+        self.last_state = None
+        self.last_action = None
+        self.last_embed = None
+        self.current_t = 0
+
+        if isinstance(self.model, Recurrent):
+            self.model.reset_state()
+
+        self.replay_buffer.stop_current_episode()
+
+class EVADoubleDQN(EVA):
+
+    def _compute_target_values(self, exp_batch):
+        batch_next_state = exp_batch['next_state']
+
+        with chainer.using_config('train', False), state_kept(self.q_function):
+            next_qout = self.q_function(batch_next_state)
+
+        target_next_qout = self.target_q_function(batch_next_state)
+
+        next_q_max = target_next_qout.evaluate_actions(
+            next_qout.greedy_actions)
+
+        batch_rewards = exp_batch['reward']
+        batch_terminal = exp_batch['is_state_terminal']
+        discount = exp_batch['discount']
+
+        return batch_rewards + discount * (1.0 - batch_terminal) * next_q_max
\ No newline at end of file
diff --git a/dkuyoshi/eva_replay_buffer.py b/dkuyoshi/eva_replay_buffer.py
index e6e9054..b61ce33 100644
--- a/dkuyoshi/eva_replay_buffer.py
+++ b/dkuyoshi/eva_replay_buffer.py
@@ -1,139 +1,146 @@
-import time
-from chainerrl.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer
-import collections
-
-class EVAReplayBuffer(ReplayBuffer):
-    def __init__(self, capacity, num_steps, key_width, xp,  M=10, T=50):
-        super().__init__(capacity, num_steps)
-        self.neighbors = M
-        self.T = T
-        self.key_width = key_width
-        self.last_n_transitions = collections.defaultdict(
-            lambda: collections.deque([], maxlen=num_steps))
-        self.xp = xp
-        self.embeddings = []
-
-    def append(self, state, action, reward, embedding, next_state=None, next_action=None,
-               is_state_terminal=False, env_id=0, **kwargs):
-        last_n_transitions = self.last_n_transitions[env_id]
-
-        experience = dict(
-            state=state,
-            action=action,
-            reward=reward,
-            embedding=embedding,
-            next_state=next_state,
-            next_action=next_action,
-            is_state_terminal=is_state_terminal,
-            **kwargs
-        )
-
-        last_n_transitions.append(experience)
-        if is_state_terminal:
-            while last_n_transitions:
-                self.memory.append(list(last_n_transitions))
-                self.embeddings += [elem['embedding'] for elem in last_n_transitions]
-                del last_n_transitions[0]
-
-            assert len(last_n_transitions) == 0
-        else:
-            if len(last_n_transitions) == self.num_steps:
-                self.memory.append(list(last_n_transitions))
-                self.embeddings += [elem['embedding'] for elem in last_n_transitions]
-
-    def stop_current_episode(self):
-            # if n-step transition hist is not full, add transition;
-            # if n-step hist is indeed full, transition has already been added;
-            if 0 < len(self.last_n_transitions) < self.num_steps:
-                self.memory.append(list(self.last_n_transitions))
-                self.embeddings += [elem['embedding'] for elem in self.last_n_transitions[0]]
-            # avoid duplicate entry
-            if 0 < len(self.last_n_transitions) <= self.num_steps:
-                del self.last_n_transitions[0]
-            while self.last_n_transitions:
-                self.memory.append(list(self.last_n_transitions))
-                self.embeddings += [elem['embedding'] for elem in self.last_n_transitions[0]]
-                del self.last_n_transitions[0]
-            assert len(self.last_n_transitions) == 0
-
-    def lookup(self, embedding):
-        """TODO: lookup(k近傍法でembeddingから軌跡の情報をreturn)する.
-        これは外から呼び出さない(private)
-        """
-        self.embeddings = self.embeddings[-self.capacity:]
-        #self.embeddings_memory = [elem[0]['embedding'] for elem in self.memory]
-        #print(self.embeddings==self.embeddings_memory)
-        # k近傍のindexを取得
-        # 例: [経験1のindex, 経験3のindex, ...]
-        I = self.index_search(embedding)
-        # 対応するindexの経験と, そのサブシーケンス(軌跡を長さTで切り取ったもの)を取り出す
-        # 例: [[経験1からの軌跡], [経験3からの軌跡], ...]
-        trajectories = [[self.memory[index + i] for i in range(min(self.T, len(self) - index))] for index in I]
-        return trajectories
-
-    def index_search(self, embedding):
-        distances = self.xp.sum((embedding - self.embeddings) ** 2, axis=1)
-        return self.xp.argsort(distances)[:self.neighbors]
-
-
-class EVAPrioritizedReplayBuffer(PrioritizedReplayBuffer):
-    def __init__(self, capacity,  key_width, xp, M=10, T=50, alpha=0.6, beta0=0.4, betasteps=2e5, eps=0.01,
-                 normalize_by_max=True, error_min=0, error_max=1, num_steps=1):
-        super().__init__(capacity, alpha, beta0, betasteps, eps, normalize_by_max, error_min, error_max, num_steps)
-
-        self.neighbors = M
-        self.T = T
-        self.key_width = key_width
-        self.xp = xp
-        self.last_n_transitions = collections.defaultdict(
-            lambda: collections.deque([], maxlen=num_steps))
-        self.embeddings = []
-        
-    def append(self, state, action, reward, embedding, next_state=None, next_action=None,
-               is_state_terminal=False, env_id=0, **kwargs):
-        last_n_transitions = self.last_n_transitions[env_id]
-
-        experience = dict(
-            state=state,
-            action=action,
-            reward=reward,
-            embedding=embedding,
-            next_state=next_state,
-            next_action=next_action,
-            is_state_terminal=is_state_terminal,
-            **kwargs
-        )
-
-        last_n_transitions.append(experience)
-        if is_state_terminal:
-            while last_n_transitions:
-                self.memory.append(list(last_n_transitions))
-                self.embeddings += [elem['embedding'] for elem in last_n_transitions]
-                del last_n_transitions[0]
-
-            assert len(last_n_transitions) == 0
-        else:
-            if len(last_n_transitions) == self.num_steps:
-                self.memory.append(list(last_n_transitions))
-                self.embeddings += [elem['embedding'] for elem in last_n_transitions]
-
-
-
-    def lookup(self, embedding):
-        """TODO: lookup(k近傍法でembeddingから軌跡の情報をreturn)する.
-        これは外から呼び出さない(private)
-        """
-        self.embeddings = [elem[0]['embedding'] for elem in self.memory.data]
-        # k近傍のindexを取得
-        # 例: [経験1のindex, 経験3のindex, ...]
-
-        I = self.index_search(embedding)
-        # 対応するindexの経験と, そのサブシーケンス(軌跡を長さTで切り取ったもの)を取り出す
-        # 例: [[経験1からの軌跡], [経験3からの軌跡], ...]
-        trajectories = [[self.memory.data[index + i] for i in range(min(self.T, len(self) - index))] for index in I]
-        return trajectories
-
-    def index_search(self, embedding):
-        distances = self.xp.sum((embedding - self.embeddings) ** 2, axis=1)
-        return self.xp.argsort(distances)[:self.neighbors]
-
+import time
+from chainer import cuda
+from chainerrl.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer
+import collections
+import numpy as np
+
+class EVAReplayBuffer(ReplayBuffer):
+    def __init__(self, capacity, num_steps, key_width, xp,  M=10, T=50):
+        super().__init__(capacity, num_steps)
+        self.neighbors = M
+        self.T = T
+        self.key_width = key_width
+        self.last_n_transitions = collections.defaultdict(
+            lambda: collections.deque([], maxlen=num_steps))
+        self.xp = xp
+        self.correct_embeddings = []
+        self.embeddings = self.xp.empty((0, key_width), dtype=self.xp.float32)
+
+    def append(self, state, action, reward, embedding, next_state=None, next_action=None,
+               is_state_terminal=False, env_id=0, **kwargs):
+        last_n_transitions = self.last_n_transitions[env_id]
+
+        experience = dict(
+            state=state,
+            action=action,
+            reward=reward,
+            embedding=embedding,
+            next_state=next_state,
+            next_action=next_action,
+            is_state_terminal=is_state_terminal,
+            **kwargs
+        )
+
+        last_n_transitions.append(experience)
+        if is_state_terminal:
+            while last_n_transitions:
+                self.memory.append(list(last_n_transitions))
+                self.correct_embeddings.append(last_n_transitions[0]['embedding'].reshape(1, -1))
+                del last_n_transitions[0]
+
+            assert len(last_n_transitions) == 0
+        else:
+            if len(last_n_transitions) == self.num_steps:
+                self.memory.append(list(last_n_transitions))
+                self.correct_embeddings.append(last_n_transitions[0]['embedding'].reshape(1, -1))
+
+    def stop_current_episode(self):
+            # if n-step transition hist is not full, add transition;
+            # if n-step hist is indeed full, transition has already been added;
+            if 0 < len(self.last_n_transitions) < self.num_steps:
+                self.memory.append(list(self.last_n_transitions))
+                self.correct_embeddings.append(self.last_n_transitions[0]['embedding'].reshape(1, -1))
+            # avoid duplicate entry
+            if 0 < len(self.last_n_transitions) <= self.num_steps:
+                del self.last_n_transitions[0]
+            while self.last_n_transitions:
+                self.memory.append(list(self.last_n_transitions))
+                self.correct_embeddings.append(self.last_n_transitions[0]['embedding'].reshape(1, -1))
+                del self.last_n_transitions[0]
+            assert len(self.last_n_transitions) == 0
+
+    def lookup(self, embedding):
+        """TODO: lookup(k近傍法でembeddingから軌跡の情報をreturn)する.
+        これは外から呼び出さない(private)
+        """
+        #self.embeddings1 = [elem[0]['embedding'] for elem in self.memory]
+        #print(self.embeddings==self.embeddings1)
+        # k近傍のindexを取得
+        # 例: [経験1のindex, 経験3のindex, ...]
+        I = cuda.to_cpu(self.index_search(embedding))
+        # 対応するindexの経験と, そのサブシーケンス(軌跡を長さTで切り取ったもの)を取り出す
+        # 例: [[経験1からの軌跡], [経験3からの軌跡], ...]
+        trajectories = [[self.memory[index + i] for i in range(min(self.T, len(self) - index))] for index in I]
+        return trajectories
+
+    def index_search(self, embedding):
+        distances = self.xp.sum((self.xp.asarray(embedding) - self.embeddings) ** 2, axis=1)
+        return self.xp.argsort(distances)[:self.neighbors]
+
+    def update_embedding(self):
+        if len(self.correct_embeddings) == 0:
+            return 0
+        embeddings = self.xp.asarray(np.concatenate(self.correct_embeddings, axis=0))
+        self.embeddings = self.xp.concatenate((self.embeddings, embeddings), axis=0)[-self.capacity:]
+        self.correct_embeddings = []
+
+class EVAPrioritizedReplayBuffer(PrioritizedReplayBuffer):
+    def __init__(self, capacity,  key_width, xp, M=10, T=50, alpha=0.6, beta0=0.4, betasteps=2e5, eps=0.01,
+                 normalize_by_max=True, error_min=0, error_max=1, num_steps=1):
+        super().__init__(capacity, alpha, beta0, betasteps, eps, normalize_by_max, error_min, error_max, num_steps)
+
+        self.neighbors = M
+        self.T = T
+        self.key_width = key_width
+        self.xp = xp
+        self.last_n_transitions = collections.defaultdict(
+            lambda: collections.deque([], maxlen=num_steps))
+        self.embeddings = []
+        
+    def append(self, state, action, reward, embedding, next_state=None, next_action=None,
+               is_state_terminal=False, env_id=0, **kwargs):
+        last_n_transitions = self.last_n_transitions[env_id]
+
+        experience = dict(
+            state=state,
+            action=action,
+            reward=reward,
+            embedding=embedding,
+            next_state=next_state,
+            next_action=next_action,
+            is_state_terminal=is_state_terminal,
+            **kwargs
+        )
+
+        last_n_transitions.append(experience)
+        if is_state_terminal:
+            while last_n_transitions:
+                self.memory.append(list(last_n_transitions))
+                self.embeddings += [elem['embedding'] for elem in last_n_transitions]
+                del last_n_transitions[0]
+
+            assert len(last_n_transitions) == 0
+        else:
+            if len(last_n_transitions) == self.num_steps:
+                self.memory.append(list(last_n_transitions))
+                self.embeddings += [elem['embedding'] for elem in last_n_transitions]
+
+
+
+    def lookup(self, embedding):
+        """TODO: lookup(k近傍法でembeddingから軌跡の情報をreturn)する.
+        これは外から呼び出さない(private)
+        """
+        self.embeddings = self.embeddings[-self.capacity:]
+        # k近傍のindexを取得
+        # 例: [経験1のindex, 経験3のindex, ...]
+
+        I = self.index_search(embedding)
+        # 対応するindexの経験と, そのサブシーケンス(軌跡を長さTで切り取ったもの)を取り出す
+        # 例: [[経験1からの軌跡], [経験3からの軌跡], ...]
+        trajectories = [[self.memory.data[index + i] for i in range(min(self.T, len(self) - index))] for index in I]
+        return trajectories
+
+    def index_search(self, embedding):
+        distances = self.xp.sum((embedding - self.embeddings) ** 2, axis=1)
+        return self.xp.argsort(distances)[:self.neighbors]
diff --git a/dkuyoshi/network.py b/dkuyoshi/network.py
index 18c5129..2e37773 100644
--- a/dkuyoshi/network.py
+++ b/dkuyoshi/network.py
@@ -1,124 +1,243 @@
-from chainer import Chain, Variable
-from chainer import links as L
-from chainer import functions as F
-
-from value_buffer import ValueBuffer
-from chainerrl.action_value import DiscreteActionValue
-
-class NN(Chain):
-    def __init__(self, obs_size, n_hidden=32, n_out=8):
-        super().__init__()
-        self.n_hidden = n_hidden
-        self.n_out = n_out
-        with self.init_scope():
-            self.l0 = L.Linear(obs_size, n_hidden)
-            self.l1 = L.Linear(n_hidden, n_out)
-
-    def __call__(self, x, test=False):
-        h = F.relu(self.l0(x))
-        h = F.relu(self.l1(h))
-        return h
-
-class QNetCart(Chain):
-    def __init__(self, obs_size, num_actions, n_hidden=8):
-        super().__init__()
-        with self.init_scope():
-            self.hout = NN(obs_size)
-            self.qout = L.Linear(n_hidden, num_actions)
-
-    def __call__(self, x, test=False):
-        self.embedding = self.hout(x)
-        return DiscreteActionValue(self.qout(F.relu(self.embedding)))
-
-class QFunctionCart(Chain):
-    def __init__(self, obs_size, num_actions, xp, LRU, n_hidden=8, lambdas=0.5, capacity=2000, num_neighbors=5):
-        super().__init__()
-        with self.init_scope():
-            self.q_func = QNetCart(obs_size, num_actions, n_hidden)
-        # non-parametricのQ値を出力するvalue bufferの初期化を呼び出す
-        self.non_q = ValueBuffer(capacity, num_neighbors, n_hidden, num_actions, xp=xp, LRU=LRU)
-        # 色々なパラメータ初期化(ラムダなど)
-        self.capacity = capacity
-        self.lambdas = lambdas
-        self.num_actions = num_actions
-
-    def __call__(self, x, eva=False, test=False):
-        """TODO: stateを受け取って, Q値を返す"""
-        # embeddingの出力
-        # embeddingからDQNのQ値出力
-        q = self.q_func(x)
-        self.embedding = self.get_embedding()
-        if not eva:
-            return q
-        # value bufferのQ値出力
-        qnp = self.non_q.get_q(self.embedding.array)
-        # Q値の足し合わせ
-        qout = self.lambdas * q.q_values + (1 - self.lambdas) * qnp
-        # Q値出力
-        return DiscreteActionValue(qout)
-
-    def get_embedding(self):
-        """embeddingだけを出力する"""
-        return self.q_func.embedding
-
-class CNN(Chain):
-    def __init__(self, n_history=4, n_hidden=256):
-        super().__init__()
-        self.n_hidden = n_hidden
-        with self.init_scope():
-            self.l1 = L.Convolution2D(n_history, 16, ksize=8, stride=4, nobias=False)
-            self.l2 = L.Convolution2D(16, 32, ksize=4, stride=2, nobias=False)
-            self.l3 = L.Convolution2D(32, 32, ksize=3, stride=1, nobias=False)
-            self.l4 = L.Linear(1568, n_hidden)
-
-    def __call__(self, x, test=False):
-        h = F.relu(self.l1(x))
-        h = F.relu(self.l2(h))
-        h = F.relu(self.l3(h))
-        h = self.l4(h)
-        return h
-
-class QNet(Chain):
-    def __init__(self, n_history, num_actions, n_hidden=256):
-        super().__init__()
-        with self.init_scope():
-            self.hout = CNN(n_history, n_hidden)
-            self.qout = L.Linear(n_hidden, num_actions)
-
-    def __call__(self, x, test=False):
-        self.embedding = self.hout(x)
-        return DiscreteActionValue(self.qout(F.relu(self.embedding)))
-
-class QFunction(Chain):
-    """EVAのQ関数"""
-
-    def __init__(self, n_history, num_actions, xp, LRU, n_hidden=256, lambdas=0.5, capacity=2000, num_neighbors=5):
-        """TODO: アーキテクチャとパラメータ定義"""
-        super().__init__()
-        with self.init_scope():
-            self.q_func = QNet(n_history, num_actions, n_hidden)
-        # non-parametricのQ値を出力するvalue bufferの初期化を呼び出す
-        self.non_q = ValueBuffer(capacity, num_neighbors, n_hidden, num_actions, xp=xp, LRU=LRU)
-        # 色々なパラメータ初期化(ラムダなど)
-        self.lambdas = lambdas
-        self.num_actions = num_actions
-        self.capacity = capacity
-
-    def __call__(self, x, eva=False, test=False):
-        """TODO: stateを受け取って, Q値を返す"""
-        #embeddingの出力
-        #embeddingからDQNのQ値出力
-        q = self.q_func(x)
-        self.embedding = self.get_embedding()
-        if not eva:
-            return q
-        #value bufferのQ値出力
-        qnp = self.non_q.get_q(self.embedding.array)
-        #Q値の足し合わせ
-        qout = self.lambdas*q.q_values + (1-self.lambdas)*qnp
-        #Q値出力
-        return DiscreteActionValue(qout)
-
-    def get_embedding(self):
-        """embeddingだけを出力する"""
-        return self.q_func.embedding
+from chainer import Chain, Variable
+from chainer import links as L
+from chainer import functions as F
+
+from value_buffer import ValueBuffer
+from chainerrl.action_value import DiscreteActionValue
+
+class NN(Chain):
+    def __init__(self, obs_size, n_hidden=32, n_out=8):
+        super().__init__()
+        self.n_hidden = n_hidden
+        self.n_out = n_out
+        with self.init_scope():
+            self.l0 = L.Linear(obs_size, n_hidden)
+            self.l1 = L.Linear(n_hidden, n_out)
+
+    def __call__(self, x, test=False):
+        h = F.relu(self.l0(x))
+        h = F.relu(self.l1(h))
+        return h
+
+class QNetCart(Chain):
+    def __init__(self, obs_size, num_actions, n_hidden=8):
+        super().__init__()
+        with self.init_scope():
+            self.hout = NN(obs_size)
+            self.qout = L.Linear(n_hidden, num_actions)
+
+    def __call__(self, x, test=False):
+        self.embedding = self.hout(x)
+        return DiscreteActionValue(self.qout(F.relu(self.embedding)))
+
+class QFunctionCart(Chain):
+    def __init__(self, obs_size, num_actions, xp, LRU, n_hidden=8, lambdas=0.5, capacity=2000, num_neighbors=5):
+        super().__init__()
+        with self.init_scope():
+            self.q_func = QNetCart(obs_size, num_actions, n_hidden)
+        # non-parametricのQ値を出力するvalue bufferの初期化を呼び出す
+        self.non_q = ValueBuffer(capacity, num_neighbors, n_hidden, num_actions, xp=xp, LRU=LRU)
+        # 色々なパラメータ初期化(ラムダなど)
+        self.capacity = capacity
+        self.lambdas = lambdas
+        self.num_actions = num_actions
+
+    def __call__(self, x, eva=False, test=False):
+        """TODO: stateを受け取って, Q値を返す"""
+        # embeddingの出力
+        # embeddingからDQNのQ値出力
+        q = self.q_func(x)
+        self.embedding = self.get_embedding()
+        if not eva:
+            return q
+        # value bufferのQ値出力
+        qnp = self.non_q.get_q(self.embedding.array)
+        # Q値の足し合わせ
+        qout = self.lambdas * q.q_values + (1 - self.lambdas) * qnp
+        # Q値出力
+        return DiscreteActionValue(qout)
+
+    def get_embedding(self):
+        """embeddingだけを出力する"""
+        return self.q_func.embedding
+
+class CNN(Chain):
+    def __init__(self, n_history=4, n_hidden=256):
+        super().__init__()
+        self.n_hidden = n_hidden
+        with self.init_scope():
+            self.l1 = L.Convolution2D(n_history, 16, ksize=8, stride=4, nobias=False)
+            self.l2 = L.Convolution2D(16, 32, ksize=4, stride=2, nobias=False)
+            self.l3 = L.Convolution2D(32, 32, ksize=3, stride=1, nobias=False)
+            self.l4 = L.Linear(1568, n_hidden)
+
+    def __call__(self, x, test=False):
+        h = F.relu(self.l1(x))
+        h = F.relu(self.l2(h))
+        h = F.relu(self.l3(h))
+        h = self.l4(h)
+        return h
+
+class QNet(Chain):
+    def __init__(self, n_history, num_actions, n_hidden=256):
+        super().__init__()
+        with self.init_scope():
+            self.hout = CNN(n_history, n_hidden)
+            self.qout = L.Linear(n_hidden, num_actions)
+
+    def __call__(self, x, test=False):
+        self.embedding = self.hout(x)
+        return DiscreteActionValue(self.qout(F.relu(self.embedding)))
+
+class QFunction(Chain):
+    """EVAのQ関数"""
+
+    def __init__(self, n_history, num_actions, xp, LRU, n_hidden=256, lambdas=0.5, capacity=2000, num_neighbors=5):
+        """TODO: アーキテクチャとパラメータ定義"""
+        super().__init__()
+        with self.init_scope():
+            self.q_func = QNet(n_history, num_actions, n_hidden)
+        # non-parametricのQ値を出力するvalue bufferの初期化を呼び出す
+        self.non_q = ValueBuffer(capacity, num_neighbors, n_hidden, num_actions, xp=xp, LRU=LRU)
+        # 色々なパラメータ初期化(ラムダなど)
+        self.lambdas = lambdas
+        self.num_actions = num_actions
+        self.capacity = capacity
+
+    def __call__(self, x, eva=False, test=False):
+        """TODO: stateを受け取って, Q値を返す"""
+        #embeddingの出力
+        #embeddingからDQNのQ値出力
+        q = self.q_func(x)
+        self.embedding = self.get_embedding()
+        if not eva:
+            return q
+        #value bufferのQ値出力
+        qnp = self.non_q.get_q(self.embedding.array)
+        #Q値の足し合わせ
+        qout = self.lambdas*q.q_values + (1-self.lambdas)*qnp
+        #Q値出力
+        return DiscreteActionValue(qout)
+
+    def get_embedding(self):
+        """embeddingだけを出力する"""
+        return self.q_func.embedding
+
+
+class DuelingQNetCart(Chain):
+    def __init__(self, obs_size, num_actions, n_hidden=8):
+        super().__init__()
+        with self.init_scope():
+            self.num_actions = num_actions
+            self.hout = NN(obs_size)
+            self.fully_layer = L.Linear(n_hidden, 8)
+            self.a_stream = L.Linear(8, num_actions)
+            self.v_stream = L.Linear(8, 1)
+
+    def __call__(self, x, test=False):
+        self.embedding = self.hout(x)
+        batch_size = x.shape[0]
+
+        l = F.relu(self.embedding)
+        activation = F.relu(self.fully_layer(l))
+        ya = self.a_stream(activation)
+        #print('a0 {}'.format(ya))
+        mean = F.reshape(F.sum(ya, axis=1) / self.num_actions, (batch_size, 1))
+        ya, mean = F.broadcast(ya, mean)
+        ya -= mean
+        self.ys_v = self.v_stream(activation)
+        #print('s0 {}'.format(ys))
+        self.ya, self.ys = F.broadcast(ya, self.ys_v)
+        q = self.ya + self.ys
+        #print('a {}'.format(ya.array))
+        #print('s {}'.format(ys))
+        #print('q {}'.format(q))
+        return DiscreteActionValue(q)
+
+class DuelingQFunctionCart(Chain):
+    def __init__(self, obs_size, num_actions, xp, LRU, n_hidden=8, lambdas=0.5, capacity=2000, num_neighbors=5):
+        super().__init__()
+        with self.init_scope():
+            self.q_func = DuelingQNetCart(obs_size, num_actions, n_hidden)
+        # non-parametricのQ値を出力するvalue bufferの初期化を呼び出す
+        self.non_q = ValueBuffer(capacity, num_neighbors, n_hidden, num_actions, xp=xp, LRU=LRU)
+        # 色々なパラメータ初期化(ラムダなど)
+        self.capacity = capacity
+        self.lambdas = lambdas
+        self.num_actions = num_actions
+
+    def __call__(self, x, eva=False, tcp=False, test=False):
+        """TODO: stateを受け取って, Q値を返す"""
+        # embeddingの出力
+        # embeddingからDQNのQ値出力
+        q = self.q_func(x)
+        self.embedding = self.get_embedding()
+        if not eva and not tcp:
+            return q
+        # value bufferのQ値出力
+        qnp = self.non_q.get_q(self.embedding.array)
+        # Q値の足し合わせ
+        qout = self.lambdas * q.q_values + (1 - self.lambdas) * qnp
+        # Q値出力
+        return DiscreteActionValue(qout)
+
+    def get_embedding(self):
+        """embeddingだけを出力する"""
+        return self.q_func.embedding
+    
+class DuelingQNet(Chain):
+    def __init__(self, n_history, num_actions, n_hidden=256):
+        super().__init__()
+        with self.init_scope():
+            self.num_actions = num_actions
+            self.hout = CNN(n_history, n_hidden)
+            #self.fully_layer = L.Linear(n_hidden, 512)
+            self.a_stream = L.Linear(n_hidden, num_actions)
+            self.v_stream = L.Linear(n_hidden, 1)
+
+    def __call__(self, x, test=False):
+        self.embedding = self.hout(x)
+        activation = F.relu(self.embedding)
+        #activation = F.relu(self.fully_layer(l))
+        batch_size = x.shape[0]
+        ya = self.a_stream(activation)
+        mean = F.reshape(F.sum(ya, axis=1) / self.num_actions, (batch_size, 1))
+        ya, mean = F.broadcast(ya, mean)
+        ya -= mean
+
+        ys = self.v_stream(activation)
+        ya, ys = F.broadcast(ya, ys)
+        q = ya + ys
+        return DiscreteActionValue(q)
+
+class DuelingQFunction(Chain):
+    def __init__(self, n_history, num_actions, xp, LRU, n_hidden=256, lambdas=0.5, capacity=2000, num_neighbors=5):
+        """TODO: アーキテクチャとパラメータ定義"""
+        super().__init__()
+        with self.init_scope():
+            self.q_func = DuelingQNet(n_history, num_actions, n_hidden)
+        # non-parametricのQ値を出力するvalue bufferの初期化を呼び出す
+        self.non_q = ValueBuffer(capacity, num_neighbors, n_hidden, num_actions, xp=xp, LRU=LRU)
+        # 色々なパラメータ初期化(ラムダなど)
+        self.lambdas = lambdas
+        self.num_actions = num_actions
+        self.capacity = capacity
+
+    def __call__(self, x, eva=False, test=False):
+        """TODO: stateを受け取って, Q値を返す"""
+        #embeddingの出力
+        #embeddingからDQNのQ値出力
+        q = self.q_func(x)
+        self.embedding = self.get_embedding()
+        if not eva:
+            return q
+        #value bufferのQ値出力
+        qnp = self.non_q.get_q(self.embedding.array)
+        #Q値の足し合わせ
+        qout = self.lambdas*q.q_values + (1-self.lambdas)*qnp
+        #Q値出力
+        return DiscreteActionValue(qout)
+
+    def get_embedding(self):
+        """embeddingだけを出力する"""
+        return self.q_func.embedding
\ No newline at end of file
diff --git a/dkuyoshi/train.py b/dkuyoshi/train.py
index a33c558..90d0de6 100644
--- a/dkuyoshi/train.py
+++ b/dkuyoshi/train.py
@@ -1,240 +1,252 @@
-from __future__ import print_function
-from __future__ import division
-from __future__ import unicode_literals
-from __future__ import absolute_import
-from builtins import *  # NOQA
-from future import standard_library
-standard_library.install_aliases()  # NOQA
-import argparse
-import os
-
-from chainer import links as L
-from chainer import optimizers
-import gym
-import gym.wrappers
-import numpy as np
-from chainer import cuda
-
-import chainerrl
-from chainerrl.action_value import DiscreteActionValue
-from chainerrl import agents
-from chainerrl import experiments
-from chainerrl import explorers
-from chainerrl import links
-from chainerrl import misc
-from chainerrl import replay_buffer
-
-from chainerrl.wrappers import atari_wrappers
-import json
-from network import QFunction
-from value_buffer import ValueBuffer
-from eva_replay_buffer import EVAReplayBuffer, EVAPrioritizedReplayBuffer
-from eva import EVA, EVADoubleDQN
-
-
-def main():
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--env', type=str, default='PongNoFrameskip-v4',
-                        help='OpenAI Atari domain to perform algorithm on.')
-    parser.add_argument('--outdir', type=str, default='results',
-                        help='Directory path to save output files.'
-                             ' If it does not exist, it will be created.')
-    parser.add_argument('--seed', type=int, default=0,
-                        help='Random seed [0, 2 ** 31)')
-    parser.add_argument('--gpu', type=int, default=0,
-                        help='GPU to use, set to -1 if no GPU.')
-    parser.add_argument('--demo', action='store_true', default=False)
-    parser.add_argument('--load', type=str, default=None)
-    parser.add_argument('--logging-level', type=int, default=20,
-                        help='Logging level. 10:DEBUG, 20:INFO etc.')
-    parser.add_argument('--render', action='store_true', default=False,
-                        help='Render env states in a GUI window.')
-    parser.add_argument('--monitor', action='store_true', default=False,
-                        help='Monitor env. Videos and additional information'
-                             ' are saved as output files.')
-    parser.add_argument('--steps', type=int, default=10 ** 7,
-                        help='Total number of timesteps to train the agent.')
-    parser.add_argument('--replay-start-size', type=int, default=4 * 10 ** 4,
-                        help='Minimum replay buffer size before ' +
-                        'performing gradient updates.')
-    parser.add_argument('--eval-n-steps', type=int, default=125000)
-    parser.add_argument('--eval-interval', type=int, default=250000)
-    parser.add_argument('--n-best-episodes', type=int, default=30)
-    parser.add_argument('--update_interval', type=int, default=4)
-    parser.add_argument('--soft-update-tau', type=float, default=1e-2)
-    parser.add_argument('--gamma', type=float, default=0.99)
-    parser.add_argument('--periodic_steps', type=int, default=20,
-                        help='backup insert period')
-    parser.add_argument('--value_buffer_neighbors', type=int, default=5,
-                        help='Number of k')
-    parser.add_argument('--lambdas', type=float, default=0.4,
-                        help='Number of λ')
-    parser.add_argument('--replay_buffer_neighbors', type=int, default=10,
-                        help='Number of M')
-    parser.add_argument('--len_trajectory', type=int, default=50,
-                        help='max length of trajectory(T)')
-    parser.add_argument('--replay_buffer_capacity', type=int, default=500000,
-                        help='Replay Buffer Capacity')
-    parser.add_argument('--value_buffer_capacity', type=int, default=2000,
-                        help='Value Buffer Capacity')
-    parser.add_argument('--minibatch_size', type=int, default=48,
-                        help='Training batch size')
-    parser.add_argument('--target_update_interval', type=int, default=2000,
-                        help='Target network period')
-    parser.add_argument('--LRU', action='store_true', default=False,
-                        help='Use LRU to store in value buffer')
-    parser.add_argument('--doubledqn', action='store_true', default=False,
-                        help='use double dqn')
-    parser.add_argument('--prioritized_replay', action='store_true', default=False)
-
-    args = parser.parse_args()
-
-    import logging
-    logging.basicConfig(level=args.logging_level)
-
-    # Set a random seed used in ChainerRL.
-    misc.set_random_seed(args.seed, gpus=(args.gpu,))
-
-    # Set different random seeds for train and test envs.
-    train_seed = args.seed
-    test_seed = 2 ** 31 - 1 - args.seed
-    dir_name = ['EVA', 'DDQN_EVA','PR', 'PR_DDQN_EVA']
-    if (args.doubledqn) and (args.prioritized_replay):
-        i = 3
-    elif args.prioritized_replay:
-        i = 2
-    elif args.doubledqn:
-        i = 1
-    else:
-        i = 0
-    args.outdir = experiments.prepare_output_dir(args, args.outdir, time_format='{}/{}/%Y%m%dT%H%M%S.%f'.format(dir_name[i], args.env))
-    print('Output files are saved in {}'.format(args.outdir))
-
-    def make_env(test):
-        # Use different random seeds for train and test envs
-        env_seed = test_seed if test else train_seed
-        env = atari_wrappers.wrap_deepmind(
-            atari_wrappers.make_atari(args.env, max_frames=None),
-            episode_life=not test,
-            clip_rewards=not test)
-        env.seed(int(env_seed))
-        if test:
-            # Randomize actions like epsilon-greedy in evaluation as well
-            env = chainerrl.wrappers.RandomizeAction(env, 0.001)
-        if args.monitor:
-            env = gym.wrappers.Monitor(
-                env, args.outdir,
-                mode='evaluation' if test else 'training')
-        if args.render:
-            env = chainerrl.wrappers.Render(env)
-        return env
-
-    env = make_env(test=False)
-    eval_env = make_env(test=True)
-
-    if args.gpu >= 0:
-        xp = cuda.cupy
-    else:
-        xp = np
-
-    n_actions = env.action_space.n
-    n_history = 4
-    q_func = QFunction( n_history, num_actions=n_actions, xp=xp, LRU=args.LRU, n_hidden=256, lambdas=args.lambdas, capacity=args.value_buffer_capacity, num_neighbors=args.value_buffer_neighbors)
-
-    # Draw the computational graph and save it in the output directory.
-    chainerrl.misc.draw_computational_graph(
-        [q_func(np.zeros((4, 84, 84), dtype=np.float32)[None])],
-        os.path.join(args.outdir, 'model'))
-
-    # Use the same hyperparameters as the Nature paper
-    opt = optimizers.Adam(0.0001)
-    opt.setup(q_func)
-
-    if args.prioritized_replay:
-        betasteps = (args.steps - args.replay_start_size) \
-                    // args.update_interval
-        rbuf = EVAPrioritizedReplayBuffer(
-            args.replay_buffer_capacity, num_steps=1, key_width=256, xp=xp, M=args.replay_buffer_neighbors,
-            T=args.len_trajectory,
-            betasteps=betasteps)
-    else:
-        rbuf = EVAReplayBuffer(args.replay_buffer_capacity, num_steps=1, key_width=256, xp=xp,
-                               M=args.replay_buffer_neighbors,
-                               T=args.len_trajectory)
-
-    explorer = explorers.LinearDecayEpsilonGreedy(
-        start_epsilon=1.0, end_epsilon=0.01,
-        decay_steps=10 ** 6,
-        random_action_func=lambda: np.random.randint(n_actions))
-
-    def phi(x):
-        # Feature extractor
-        return np.asarray(x, dtype=np.float32) / 255
-
-    if args.doubledqn:
-        Agent = EVADoubleDQN
-    else:
-        Agent = EVA
-
-    agent = Agent(q_func, opt, rbuf, gamma=args.gamma,
-                  explorer=explorer, gpu=args.gpu, replay_start_size=args.replay_start_size,
-                  minibatch_size=args.minibatch_size, update_interval=args.update_interval,
-                  target_update_interval=args.target_update_interval, clip_delta=True,
-                  phi=phi,
-                  target_update_method='hard',
-                  soft_update_tau=args.soft_update_tau,
-                  n_times_update=1, average_q_decay=0.999,
-                  average_loss_decay=0.99,
-                  batch_accumulator='mean', episodic_update=False,
-                  episodic_update_len=16,
-                  len_trajectory=args.len_trajectory,
-                  periodic_steps=args.periodic_steps
-                  )
-
-    if args.load:
-        agent.load(args.load)
-
-    if args.demo:
-        eval_stats = experiments.eval_performance(
-            env=eval_env,
-            agent=agent,
-            n_steps=args.eval_n_steps,
-            n_episodes=None)
-        print('n_episodes: {} mean: {} median: {} stdev {}'.format(
-            eval_stats['episodes'],
-            eval_stats['mean'],
-            eval_stats['median'],
-            eval_stats['stdev']))
-    else:
-        experiments.train_agent_with_evaluation(
-            agent=agent, env=env, steps=args.steps,
-            eval_n_steps=args.eval_n_steps,
-            eval_n_episodes=None,
-            eval_interval=args.eval_interval,
-            outdir=args.outdir,
-            save_best_so_far_agent=True,
-            eval_env=eval_env,
-        )
-
-        dir_of_best_network = os.path.join(args.outdir, "best")
-        agent.load(dir_of_best_network)
-
-        # run 30 evaluation episodes, each capped at 5 mins of play
-        stats = experiments.evaluator.eval_performance(
-            env=eval_env,
-            agent=agent,
-            n_steps=None,
-            n_episodes=args.n_best_episodes,
-            max_episode_len=4500,
-            logger=None)
-        with open(os.path.join(args.outdir, 'bestscores.json'), 'w') as f:
-            # temporary hack to handle python 2/3 support issues.
-            # json dumps does not support non-string literal dict keys
-            json_stats = json.dumps(stats)
-            print(str(json_stats), file=f)
-        print("The results of the best scoring network:")
-        for stat in stats:
-            print(str(stat) + ":" + str(stats[stat]))
-
-if __name__ == '__main__':
-    main()
+from __future__ import print_function
+from __future__ import division
+from __future__ import unicode_literals
+from __future__ import absolute_import
+from builtins import *  # NOQA
+from future import standard_library
+standard_library.install_aliases()  # NOQA
+import argparse
+import os
+
+from chainer import links as L
+from chainer import optimizers
+import gym
+import gym.wrappers
+import numpy as np
+from chainer import cuda
+
+import chainerrl
+from chainerrl.action_value import DiscreteActionValue
+from chainerrl import agents
+from chainerrl import experiments
+from chainerrl import explorers
+from chainerrl import links
+from chainerrl import misc
+from chainerrl import replay_buffer
+
+from chainerrl.wrappers import atari_wrappers
+import json
+from network import QFunction, DuelingQFunction
+from value_buffer import ValueBuffer
+from eva_replay_buffer import EVAReplayBuffer, EVAPrioritizedReplayBuffer
+from eva import EVA, EVADoubleDQN
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--env', type=str, default='PongNoFrameskip-v4',
+                        help='OpenAI Atari domain to perform algorithm on.')
+    parser.add_argument('--outdir', type=str, default='results',
+                        help='Directory path to save output files.'
+                             ' If it does not exist, it will be created.')
+    parser.add_argument('--seed', type=int, default=0,
+                        help='Random seed [0, 2 ** 31)')
+    parser.add_argument('--gpu', type=int, default=0,
+                        help='GPU to use, set to -1 if no GPU.')
+    parser.add_argument('--demo', action='store_true', default=False)
+    parser.add_argument('--load', type=str, default=None)
+    parser.add_argument('--logging-level', type=int, default=20,
+                        help='Logging level. 10:DEBUG, 20:INFO etc.')
+    parser.add_argument('--render', action='store_true', default=False,
+                        help='Render env states in a GUI window.')
+    parser.add_argument('--monitor', action='store_true', default=False,
+                        help='Monitor env. Videos and additional information'
+                             ' are saved as output files.')
+    parser.add_argument('--steps', type=int, default=10 ** 7,
+                        help='Total number of timesteps to train the agent.')
+    parser.add_argument('--replay-start-size', type=int, default=4 * 10 ** 4,
+                        help='Minimum replay buffer size before ' +
+                        'performing gradient updates.')
+    parser.add_argument('--eval-n-steps', type=int, default=125000)
+    parser.add_argument('--eval-interval', type=int, default=250000)
+    parser.add_argument('--n-best-episodes', type=int, default=30)
+    parser.add_argument('--update_interval', type=int, default=4)
+    parser.add_argument('--soft-update-tau', type=float, default=1e-2)
+    parser.add_argument('--gamma', type=float, default=0.99)
+    parser.add_argument('--periodic_steps', type=int, default=20,
+                        help='backup insert period')
+    parser.add_argument('--value_buffer_neighbors', type=int, default=5,
+                        help='Number of k')
+    parser.add_argument('--lambdas', type=float, default=0.4,
+                        help='Number of λ')
+    parser.add_argument('--replay_buffer_neighbors', type=int, default=10,
+                        help='Number of M')
+    parser.add_argument('--len_trajectory', type=int, default=50,
+                        help='max length of trajectory(T)')
+    parser.add_argument('--replay_buffer_capacity', type=int, default=500000,
+                        help='Replay Buffer Capacity')
+    parser.add_argument('--value_buffer_capacity', type=int, default=2000,
+                        help='Value Buffer Capacity')
+    parser.add_argument('--minibatch_size', type=int, default=48,
+                        help='Training batch size')
+    parser.add_argument('--target_update_interval', type=int, default=2000,
+                        help='Target network period')
+    parser.add_argument('--LRU', action='store_true', default=False,
+                        help='Use LRU to store in value buffer')
+    parser.add_argument('--doubledqn', action='store_true', default=False,
+                        help='use double dqn')
+    parser.add_argument('--prioritized_replay', action='store_true', default=False)
+    parser.add_argument('--dueling', action='store_true', default=False,
+                        help='use dueling dqn')
+
+    args = parser.parse_args()
+
+    import logging
+    logging.basicConfig(level=args.logging_level)
+
+    # Set a random seed used in ChainerRL.
+    misc.set_random_seed(args.seed, gpus=(args.gpu,))
+
+    # Set different random seeds for train and test envs.
+    train_seed = args.seed
+    test_seed = 2 ** 31 - 1 - args.seed
+    dir_name = ['EVA', 'DDQN_EVA','PR_EVA', 'PR_DDQN_EVA', 'DuelingEVA', 'DDEVA']
+    if args.dueling and args.doubledqn:
+        i = 5
+    elif args.dueling:
+        i = 4
+    elif (args.doubledqn) and (args.prioritized_replay):
+        i = 3
+    elif args.prioritized_replay:
+        i = 2
+    elif args.doubledqn:
+        i = 1
+    else:
+        i = 0
+    args.outdir = experiments.prepare_output_dir(args, args.outdir, time_format='{}/{}/seed{}/%Y%m%dT%H%M%S.%f'.format(dir_name[i], args.env, args.seed))
+    print('Output files are saved in {}'.format(args.outdir))
+
+    def make_env(test):
+        # Use different random seeds for train and test envs
+        env_seed = test_seed if test else train_seed
+        env = atari_wrappers.wrap_deepmind(
+            atari_wrappers.make_atari(args.env, max_frames=None),
+            episode_life=not test,
+            clip_rewards=not test)
+        env.seed(int(env_seed))
+        if test:
+            # Randomize actions like epsilon-greedy in evaluation as well
+            env = chainerrl.wrappers.RandomizeAction(env, 0.001)
+        if args.monitor:
+            env = gym.wrappers.Monitor(
+                env, args.outdir,
+                mode='evaluation' if test else 'training')
+        if args.render:
+            env = chainerrl.wrappers.Render(env)
+        return env
+
+    env = make_env(test=False)
+    eval_env = make_env(test=True)
+
+    if args.gpu >= 0:
+        xp = cuda.cupy
+    else:
+        xp = np
+
+    n_actions = env.action_space.n
+    n_history = 4
+    if args.dueling:
+        q_func =   DuelingQFunction(n_history, num_actions=n_actions, xp=xp, LRU=args.LRU, n_hidden=256, lambdas=args.lambdas, capacity=args.value_buffer_capacity,
+                       num_neighbors=args.value_buffer_neighbors)
+
+    else:
+        q_func = QFunction(n_history, num_actions=n_actions, xp=xp, LRU=args.LRU, n_hidden=256, lambdas=args.lambdas, capacity=args.value_buffer_capacity,
+                       num_neighbors=args.value_buffer_neighbors)
+
+    # Draw the computational graph and save it in the output directory.
+    chainerrl.misc.draw_computational_graph(
+        [q_func(np.zeros((4, 84, 84), dtype=np.float32)[None])],
+        os.path.join(args.outdir, 'model'))
+
+    # Use the same hyperparameters as the Nature paper
+    opt = optimizers.Adam(0.0001)
+    opt.setup(q_func)
+
+    if args.prioritized_replay:
+        betasteps = (args.steps - args.replay_start_size) \
+                    // args.update_interval
+        rbuf = EVAPrioritizedReplayBuffer(
+            args.replay_buffer_capacity, num_steps=1, key_width=256, xp=xp, M=args.replay_buffer_neighbors,
+            T=args.len_trajectory,
+            betasteps=betasteps)
+    else:
+        rbuf = EVAReplayBuffer(args.replay_buffer_capacity, num_steps=1, key_width=256, xp=xp,
+                               M=args.replay_buffer_neighbors,
+                               T=args.len_trajectory)
+
+    explorer = explorers.LinearDecayEpsilonGreedy(
+        start_epsilon=1.0, end_epsilon=0.01,
+        decay_steps=10 ** 6,
+        random_action_func=lambda: np.random.randint(n_actions))
+
+    def phi(x):
+        # Feature extractor
+        return np.asarray(x, dtype=np.float32) / 255
+
+    if args.doubledqn:
+        Agent = EVADoubleDQN
+    else:
+        Agent = EVA
+
+    agent = Agent(q_func, opt, rbuf, gamma=args.gamma,
+                  explorer=explorer, gpu=args.gpu, replay_start_size=args.replay_start_size,
+                  minibatch_size=args.minibatch_size, update_interval=args.update_interval,
+                  target_update_interval=args.target_update_interval, clip_delta=True,
+                  phi=phi,
+                  target_update_method='hard',
+                  soft_update_tau=args.soft_update_tau,
+                  n_times_update=1, average_q_decay=0.999,
+                  average_loss_decay=0.99,
+                  batch_accumulator='mean', episodic_update=False,
+                  episodic_update_len=16,
+                  len_trajectory=args.len_trajectory,
+                  periodic_steps=args.periodic_steps
+                  )
+
+    if args.load:
+        agent.load(args.load)
+
+    if args.demo:
+        eval_stats = experiments.eval_performance(
+            env=eval_env,
+            agent=agent,
+            n_steps=args.eval_n_steps,
+            n_episodes=None)
+        print('n_episodes: {} mean: {} median: {} stdev {}'.format(
+            eval_stats['episodes'],
+            eval_stats['mean'],
+            eval_stats['median'],
+            eval_stats['stdev']))
+    else:
+        experiments.train_agent_with_evaluation(
+            agent=agent, env=env, steps=args.steps,
+            eval_n_steps=args.eval_n_steps,
+            eval_n_episodes=None,
+            eval_interval=args.eval_interval,
+            outdir=args.outdir,
+            save_best_so_far_agent=True,
+            eval_env=eval_env,
+        )
+
+        dir_of_best_network = os.path.join(args.outdir, "best")
+        agent.load(dir_of_best_network)
+
+        # run 30 evaluation episodes, each capped at 5 mins of play
+        stats = experiments.evaluator.eval_performance(
+            env=eval_env,
+            agent=agent,
+            n_steps=None,
+            n_episodes=args.n_best_episodes,
+            max_episode_len=4500,
+            logger=None)
+        with open(os.path.join(args.outdir, 'bestscores.json'), 'w') as f:
+            # temporary hack to handle python 2/3 support issues.
+            # json dumps does not support non-string literal dict keys
+            json_stats = json.dumps(stats)
+            print(str(json_stats), file=f)
+        print("The results of the best scoring network:")
+        for stat in stats:
+            print(str(stat) + ":" + str(stats[stat]))
+
+if __name__ == '__main__':
+    main()
\ No newline at end of file
diff --git a/dkuyoshi/value_buffer.py b/dkuyoshi/value_buffer.py
index 50f00fc..0988728 100644
--- a/dkuyoshi/value_buffer.py
+++ b/dkuyoshi/value_buffer.py
@@ -1,71 +1,67 @@
-
-class ValueBuffer(object):
-    """non-parametricQ値を出力するためのbuffer"""
-
-    def __init__(self, capacity, k,  key_width, num_actions, xp, LRU=False):
-        """TODO: パラメータ定義"""
-
-        # 色々なパラメータの初期化
-        self.capacity = capacity
-        self.neighbors = k
-        self.key_width = key_width
-        self.num_actions = num_actions
-        self.xp = xp
-        self.LRU = LRU
-        self.xp = xp
-
-        if self.LRU:
-            self.lru_timestamps = self.xp.empty(self.capacity, dtype=self.xp.int32)
-            self.current_timestamp = 0
-            self.i = 0
-            self.embeddings = self.xp.empty((self.capacity, key_width), dtype=self.xp.float32)
-            self.values = self.xp.empty((self.capacity, num_actions), dtype=self.xp.float32)
-        else:
-            self.embeddings = self.xp.empty((0, key_width), dtype=self.xp.float32)
-            self.values = self.xp.empty((0, num_actions), dtype=self.xp.float32)
-
-    def store(self, embeddings, values):
-        if self.LRU:
-            self._lru_store(embeddings, values)
-        else:
-            self._normal_store(embeddings, values)
-
-    def _normal_store(self, embeddings, values):
-        """TODO: embeddingと対応するQ値を保存する
-        これは外から呼び出さない(private)
-        """
-        self.embeddings = self.xp.vstack((self.embeddings, embeddings))[-self.capacity:]
-        self.values = self.xp.vstack((self.values, values))[-self.capacity:]
-        # 戻り値はなし (必要ならつける)
-        return
-
-    def _lru_store(self, embeddings, values):
-        for embedding, value in zip(embeddings, values):
-            if self.i < self.capacity:
-                self.embeddings[self.i] = embedding
-                self.values[self.i] = value
-                self.lru_timestamps[self.i] = self.current_timestamp
-                self.i += 1
-                self.current_timestamp += 1
-            else:
-                index = self.xp.argmin(self.lru_timestamps)
-                self.embeddings[index] = embedding
-                self.values[index] = value
-                self.lru_timestamps[index] = self.current_timestamp
-                self.current_timestamp += 1
-        return
-
-    def get_q(self, embedding):
-        indices = self.index_search(embedding)
-        if self.LRU:
-            for index in indices:
-                self.lru_timestamps[index] = self.current_timestamp
-                self.current_timestamp += 1
-        values = self.values[indices]
-        return self.xp.mean(values, axis=0)
-
-    def index_search(self, embedding):
-        distances = self.xp.sum((embedding - self.embeddings)**2, axis=1)
-        return self.xp.argsort(distances)[:self.neighbors]
-
-
+
+class ValueBuffer(object):
+    """non-parametricQ値を出力するためのbuffer"""
+
+    def __init__(self, capacity, k,  key_width, num_actions, xp, LRU=False):
+        """TODO: パラメータ定義"""
+
+        # 色々なパラメータの初期化
+        self.capacity = capacity
+        self.neighbors = k
+        self.key_width = key_width
+        self.num_actions = num_actions
+        self.xp = xp
+        self.LRU = LRU
+        self.xp = xp
+
+        if self.LRU:
+            self.lru_timestamps = self.xp.empty(self.capacity, dtype=self.xp.int32)
+            self.current_timestamp = 0
+            self.i = 0
+            self.embeddings = self.xp.empty((self.capacity, key_width), dtype=self.xp.float32)
+            self.values = self.xp.empty((self.capacity, num_actions), dtype=self.xp.float32)
+        else:
+            self.embeddings = self.xp.empty((0, key_width), dtype=self.xp.float32)
+            self.values = self.xp.empty((0, num_actions), dtype=self.xp.float32)
+
+    def store(self, embeddings, values):
+        if self.LRU:
+            self._lru_store(embeddings, values)
+        else:
+            self._normal_store(embeddings, values)
+
+    def _normal_store(self, embeddings, values):
+        """TODO: embeddingと対応するQ値を保存する
+        これは外から呼び出さない(private)
+        """
+        self.embeddings = self.xp.concatenate((self.embeddings, embeddings), axis=0)[-self.capacity:]
+        self.values = self.xp.concatenate((self.values, values), axis=0)[-self.capacity:]
+
+    def _lru_store(self, embeddings, values):
+        for embedding, value in zip(embeddings, values):
+            if self.i < self.capacity:
+                self.embeddings[self.i] = embedding
+                self.values[self.i] = value
+                self.lru_timestamps[self.i] = self.current_timestamp
+                self.i += 1
+                self.current_timestamp += 1
+            else:
+                index = self.xp.argmin(self.lru_timestamps)
+                self.embeddings[index] = embedding
+                self.values[index] = value
+                self.lru_timestamps[index] = self.current_timestamp
+                self.current_timestamp += 1
+        return
+
+    def get_q(self, embedding):
+        indices = self.index_search(embedding)
+        if self.LRU:
+            for index in indices:
+                self.lru_timestamps[index] = self.current_timestamp
+                self.current_timestamp += 1
+        values = self.xp.asarray(self.values[indices])
+        return self.xp.mean(values, axis=0)
+
+    def index_search(self, embedding):
+        distances = self.xp.sum((self.xp.asarray(embedding) - self.embeddings)**2, axis=1)
+        return self.xp.argsort(distances)[:self.neighbors]
\ No newline at end of file
